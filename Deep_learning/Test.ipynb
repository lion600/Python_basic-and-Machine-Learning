{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the architecture of Spark:**\n",
        "\n",
        "Apache Spark is an open-source distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Its architecture is based on a master-slave architecture known as the Spark Master-Slave Architecture:\n",
        "\n",
        "**Driver Program:** The main program that executes the user’s main function and creates the SparkContext.\n",
        "\n",
        "**Spark Context**: The SparkContext object coordinates the execution of the Spark job on a cluster, providing a way to connect to the cluster, create RDDs, and broadcast variables.\n",
        "\n",
        "**Cluster Manager:** The cluster manager allocates resources across applications.\n",
        "\n",
        "**Workers**: The nodes in the cluster that execute tasks.\n",
        "\n",
        "**Executors:** Processes launched by the worker nodes to run tasks. Each executor is responsible for executing tasks on a particular node in the cluster."
      ],
      "metadata": {
        "id": "nUWpiSgvpGcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. Explain activation function:**\n",
        "\n",
        "An activation function in a neural network defines the output of a neuron given its input. It introduces non-linearity to the neural network, allowing it to learn complex patterns in the data. Activation functions help in determining whether a neuron should be activated or not, based on whether the neuron's input is relevant for the given context."
      ],
      "metadata": {
        "id": "aCzptyAPqHDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. List different types of activation function with their formula**\n",
        "\n",
        "Sure, here are several common types of activation functions used in neural networks along with their formulas:\n",
        "\n",
        "1. **Sigmoid Function**:\n",
        "   Formula:  f(x) = frac{1}{1 + e^{-x}}\n",
        "   Range: (0, 1)\n",
        "\n",
        "   Description: Sigmoid function squashes the input values between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        "2. **ReLU (Rectified Linear Unit)**:\n",
        "   Formula: f(x) = max(0, x)\n",
        "\n",
        "   Range: [0, +∞ ]\n",
        "\n",
        "   Description: ReLU function returns 0 for negative inputs and the input value for positive inputs, introducing sparsity and accelerating convergence.\n",
        "\n",
        "3. **Leaky ReLU**:\n",
        "   Formula: LRELU (X) = X IF X>0 , AX OTHERWISE\n",
        "   \n",
        "   Range: (-∞, +∞)\n",
        "\n",
        "   Description: Leaky ReLU avoids zero gradients for negative inputs, addressing the \"dying ReLU\" problem.\n",
        "\n",
        "4. **Tanh (Hyperbolic Tangent)**:\n",
        "\n",
        "   Formula:  f(x) = {e^x - e^{-x}}/{e^x + e^{-x}}\n",
        "\n",
        "   Range: (-1, 1)\n",
        "\n",
        "   Description: Tanh function squashes the input values between -1 and 1, enabling better convergence by centering the data around zero.\n",
        "\n",
        "5. **Softmax**:\n",
        "\n",
        "   Formula: f(x_i) = e^{x_i}}\\sum_{j} e^{x_j} (for the ith) element of the output vector)\n",
        "\n",
        "   Range: (0, 1) and the sum of all output values equals 1\n",
        "\n",
        "   Description: Softmax function is typically used in the output layer of a neural network for multi-class classification tasks, as it converts the raw scores into probabilities.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pUoGIyf3qmW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain Hybrid Inheritance with Code.**\n",
        "\n",
        "Hybrid inheritance refers to a combination of multiple types of inheritance in a single class hierarchy. It can involve multiple inheritance (where a class inherits from more than one base class) and multilevel inheritance (where a derived class becomes the base class for another class).\n",
        "example:"
      ],
      "metadata": {
        "id": "PlxkL_g_uQUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class A:\n",
        "    def method_A(self):\n",
        "        print(\"Method of class A\")\n",
        "\n",
        "class B(A):\n",
        "    def method_B(self):\n",
        "        print(\"Method of class B\")\n",
        "\n",
        "class C:\n",
        "    def method_C(self):\n",
        "        print(\"Method of class C\")\n",
        "\n",
        "class D(B, C):\n",
        "    def method_D(self):\n",
        "        print(\"Method of class D\")\n",
        "\n",
        "class E(D):\n",
        "    def method_E(self):\n",
        "        print(\"Method of class E\")\n",
        "\n",
        "obj_E = E()\n",
        "\n",
        "obj_E.method_A()\n",
        "obj_E.method_B()\n",
        "obj_E.method_C()\n",
        "obj_E.method_D()\n",
        "obj_E.method_E()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRRSUYNqqk2N",
        "outputId": "3e3f07b9-1e06-42dc-a270-33cf4ccfe2d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method of class A\n",
            "Method of class B\n",
            "Method of class C\n",
            "Method of class D\n",
            "Method of class E\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain Neural Networks **\n",
        "\n",
        "Neural networks are a type of machine learning model inspired by the structure and functioning of the human brain. They consist of interconnected nodes, or neurons, organized into layers. Each neuron processes input data, applies a set of weights, and produces an output. Neural networks are capable of learning complex patterns and relationships within data, making them powerful tools for tasks such as classification, regression, and pattern recognition.\n",
        "\n",
        "Here are the key components and concepts of neural networks:\n",
        "\n",
        "1. **Neurons (Nodes)**: Neurons are the fundamental units of a neural network. They receive input signals, perform a computation, and produce an output signal. In artificial neural networks, neurons are typically represented as mathematical functions.\n",
        "\n",
        "2. **Layers**: Neural networks are organized into layers, which are collections of neurons that process input data together. The three main types of layers are:\n",
        "   - **Input Layer**: The input layer receives the initial data and passes it on to the next layer.\n",
        "   - **Hidden Layers**: Hidden layers are intermediary layers between the input and output layers. They perform complex computations and feature extraction.\n",
        "   - **Output Layer**: The output layer produces the final output of the neural network's prediction.\n",
        "\n",
        "3. **Weights and Biases**: Each connection between neurons in adjacent layers is associated with a weight, which represents the strength of the connection. Additionally, each neuron has a bias, which allows it to shift the activation function.\n",
        "\n",
        "4. **Activation Function**: An activation function determines the output of a neuron given its input. It introduces non-linearity into the neural network, enabling it to learn complex relationships in the data. Common activation functions include sigmoid, ReLU, tanh, and softmax.\n",
        "\n",
        "5. **Feedforward Propagation**: Feedforward propagation is the process by which input data is passed through the network, layer by layer, to produce an output. The output is then compared to the actual target values to compute the error.\n",
        "\n",
        "6. **Backpropagation**: Backpropagation is an algorithm used to train neural networks by adjusting the weights and biases based on the computed error. It involves calculating the gradient of the error with respect to each weight and bias and using this information to update the parameters via optimization algorithms like gradient descent.\n",
        "\n",
        "7. **Training**: Training a neural network involves feeding it with labeled data (input-output pairs) and adjusting its parameters (weights and biases) iteratively to minimize the difference between the predicted output and the actual output.\n",
        "\n",
        "8. **Deep Learning**: Deep learning refers to the use of neural networks with multiple hidden layers (deep architectures) to learn intricate patterns from large amounts of data. Deep learning has achieved remarkable success in various fields, including computer vision, natural language processing, and speech recognition.\n",
        "\n"
      ],
      "metadata": {
        "id": "N9kvSs0Tux6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network from scratch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases for the hidden layer and output layer\n",
        "        self.weights_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_output = np.random.randn(hidden_size, output_size)\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Sigmoid activation function\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        # Derivative of the sigmoid function\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward propagation\n",
        "        self.hidden_input = np.dot(x, self.weights_hidden) + self.bias_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
        "        self.output = np.dot(self.hidden_output, self.weights_output) + self.bias_output\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, x, y, output, learning_rate):\n",
        "        # Backpropagation\n",
        "        error = output - y\n",
        "        d_output = error\n",
        "        d_weights_output = np.dot(self.hidden_output.T, d_output)\n",
        "        d_bias_output = np.sum(d_output, axis=0, keepdims=True)\n",
        "\n",
        "        error_hidden = np.dot(d_output, self.weights_output.T)\n",
        "        d_hidden = error_hidden * self.sigmoid_derivative(self.hidden_output)\n",
        "        d_weights_hidden = np.dot(x.T, d_hidden)\n",
        "        d_bias_hidden = np.sum(d_hidden, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden -= learning_rate * d_weights_hidden\n",
        "        self.bias_hidden -= learning_rate * d_bias_hidden\n",
        "        self.weights_output -= learning_rate * d_weights_output\n",
        "        self.bias_output -= learning_rate * d_bias_output\n",
        "\n",
        "    def train(self, x, y, epochs, learning_rate):\n",
        "        # Training the neural network\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(x)\n",
        "            # Backward pass\n",
        "            self.backward(x, y, output, learning_rate)\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Making predictions\n",
        "        return self.forward(x)\n",
        "\n",
        "# Example usage\n",
        "# Define input, output, and hidden layer sizes\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Create a neural network instance\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Example training data\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X_train, y_train, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "predictions = nn.predict(X_test)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Ex0WNSpiPV",
        "outputId": "4f3762e7-d940-48cb-aa9f-ef579b55dc97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [[1.55431223e-15]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [5.55111512e-15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsN6MbRuzIPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}